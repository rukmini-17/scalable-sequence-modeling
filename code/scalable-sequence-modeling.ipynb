{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_1jBnMJfmJb",
        "outputId": "2e042a95-0726-4acb-b995-d9ef0e55f104"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Using device: cuda\n",
            "--- Loading tiny_shakespeare (char) ---\n",
            "--- Loading wikitext-2 (word) ---\n",
            "--- Loading ptb (word) ---\n",
            "\n",
            "SUCCESS: All datasets loaded.\n",
            "Shakespeare Vocab: 65\n",
            "WikiText Vocab: 33278\n",
            "PTB Vocab: 10000\n"
          ]
        }
      ],
      "source": [
        "# @title 1. Setup & Data Loading\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import math\n",
        "import time\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. setup environment\n",
        "drive.mount('/content/drive')\n",
        "BASE_DIR = '/content/drive/My Drive/HW6_Project'\n",
        "\n",
        "os.makedirs(f\"{BASE_DIR}/logs\", exist_ok=True)       # For CSVs\n",
        "os.makedirs(f\"{BASE_DIR}/images\", exist_ok=True)     # For Plots\n",
        "os.makedirs(f\"{BASE_DIR}/checkpoints\", exist_ok=True) # For Models\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 2. tokenizers\n",
        "class CharTokenizer:\n",
        "    def __init__(self, text):\n",
        "        chars = sorted(list(set(text)))\n",
        "        self.vocab_size = len(chars)\n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
        "    def encode(self, s): return [self.stoi[c] for c in s]\n",
        "    def decode(self, l): return ''.join([self.itos[i] for i in l])\n",
        "\n",
        "class WordTokenizer:\n",
        "    def __init__(self, file_path):\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "        words = text.replace('\\n', ' <eos> ').split()\n",
        "        self.vocab = sorted(list(set(words)))\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        self.stoi = { w:i for i,w in enumerate(self.vocab) }\n",
        "        self.itos = { i:w for i,w in enumerate(self.vocab) }\n",
        "    def encode(self, s_list): return [self.stoi.get(w, 0) for w in s_list]\n",
        "    def decode(self, l): return ' '.join([self.itos[i] for i in l])\n",
        "\n",
        "# 3. data loader\n",
        "def load_data_from_drive(dataset_name, mode='word'):\n",
        "    folder = f\"{BASE_DIR}/datasets/{dataset_name}\"\n",
        "    print(f\"--- Loading {dataset_name} ({mode}) ---\")\n",
        "\n",
        "    # create tokenizer from TRAIN set\n",
        "    if mode == 'char':\n",
        "        # check if split files exist, otherwise use input.txt\n",
        "        if os.path.exists(f\"{folder}/train.txt\"):\n",
        "            with open(f\"{folder}/train.txt\", 'r') as f: train_text = f.read()\n",
        "            tokenizer = CharTokenizer(train_text)\n",
        "        else:\n",
        "            with open(f\"{folder}/input.txt\", 'r') as f: train_text = f.read()\n",
        "            tokenizer = CharTokenizer(train_text)\n",
        "    else:\n",
        "        tokenizer = WordTokenizer(f\"{folder}/train.txt\")\n",
        "\n",
        "    # helper to load file -> tensor\n",
        "    def file_to_tensor(fname):\n",
        "        path = f\"{folder}/{fname}\"\n",
        "        if not os.path.exists(path):\n",
        "            if fname == 'train.txt' and os.path.exists(f\"{folder}/input.txt\"):\n",
        "                 with open(f\"{folder}/input.txt\", 'r') as f: content = f.read()\n",
        "                 split_idx = int(0.9 * len(content))\n",
        "                 content = content[:split_idx]\n",
        "            else:\n",
        "                 print(f\"Warning: {fname} not found in {folder}\")\n",
        "                 return torch.tensor([], dtype=torch.long)\n",
        "        else:\n",
        "            with open(path, 'r') as f: content = f.read()\n",
        "\n",
        "        if mode == 'char':\n",
        "            encoded = tokenizer.encode(content)\n",
        "        else:\n",
        "            encoded = tokenizer.encode(content.replace('\\n', ' <eos> ').split())\n",
        "        return torch.tensor(encoded, dtype=torch.long)\n",
        "\n",
        "    return {\n",
        "        'train': file_to_tensor('train.txt'),\n",
        "        'val':   file_to_tensor('valid.txt'),\n",
        "        'test':  file_to_tensor('test.txt'),\n",
        "        'tokenizer': tokenizer\n",
        "    }\n",
        "\n",
        "# 4. load data\n",
        "try:\n",
        "    shakespeare = load_data_from_drive('tiny_shakespeare', mode='char')\n",
        "    wiki = load_data_from_drive('wikitext-2', mode='word')\n",
        "    ptb = load_data_from_drive('ptb', mode='word')\n",
        "    print(\"\\nSUCCESS: All datasets loaded.\")\n",
        "    print(f\"Shakespeare Vocab: {shakespeare['tokenizer'].vocab_size}\")\n",
        "    print(f\"WikiText Vocab: {wiki['tokenizer'].vocab_size}\")\n",
        "    print(f\"PTB Vocab: {ptb['tokenizer'].vocab_size}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nERROR: {e}\")\n",
        "    print(\"Please check your Drive folder structure.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Model Definitions (Fixed Mamba Einsum & Split)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "# --- A. HELPER MODULES ---\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "# --- B. LINEAR ---\n",
        "class LinearModel(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embd, block_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "    def forward(self, idx, targets=None):\n",
        "        x = self.token_embedding(idx)\n",
        "        logits = self.lm_head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B, T, C = logits.shape\n",
        "            loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
        "        return logits, loss\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, _ = self(idx)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "# --- C. MLP ---\n",
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embd, block_size, n_hidden=256):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_hidden), nn.ReLU(),\n",
        "            nn.Linear(n_hidden, n_hidden), nn.ReLU(),\n",
        "            nn.Linear(n_hidden, n_embd)\n",
        "        )\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "    def forward(self, idx, targets=None):\n",
        "        x = self.token_embedding(idx)\n",
        "        x = self.mlp(x)\n",
        "        logits = self.lm_head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B, T, C = logits.shape\n",
        "            loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
        "        return logits, loss\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, _ = self(idx)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "# --- D. ATTENTION ---\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size, n_embd, block_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        v = self.value(x)\n",
        "        return wei @ v\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size, n_embd, block_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        return self.proj(out)\n",
        "\n",
        "# --- E. SELF-ATTENTION ONLY ---\n",
        "class SelfAttentionModel(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embd, block_size, n_head):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding = nn.Embedding(block_size, n_embd)\n",
        "        self.sa_head = MultiHeadAttention(n_head, n_embd//n_head, n_embd, block_size)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "        self.block_size = block_size\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding(idx)\n",
        "        pos_emb = self.position_embedding(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.sa_head(x)\n",
        "        logits = self.lm_head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B, T, C = logits.shape\n",
        "            loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
        "        return logits, loss\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "# --- F. TRANSFORMER ---\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, block_size):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size, n_embd, block_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = LayerNorm(n_embd, bias=True)\n",
        "        self.ln2 = LayerNorm(n_embd, bias=True)\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embd, block_size, n_head, n_layer):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[TransformerBlock(n_embd, n_head, block_size) for _ in range(n_layer)])\n",
        "        self.ln_f = LayerNorm(n_embd, bias=True)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "        self.block_size = block_size\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding(idx)\n",
        "        pos_emb = self.position_embedding(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B, T, C = logits.shape\n",
        "            loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
        "        return logits, loss\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "# --- G. MAMBA ---\n",
        "class MinimalMambaBlock(nn.Module):\n",
        "    def __init__(self, n_embd, d_state=16, d_conv=4, expand=2):\n",
        "        super().__init__()\n",
        "        self.d_inner = int(expand * n_embd)\n",
        "        self.dt_rank = math.ceil(n_embd / 16)\n",
        "        self.d_state = d_state\n",
        "\n",
        "        self.in_proj = nn.Linear(n_embd, self.d_inner * 2, bias=False)\n",
        "        self.conv1d = nn.Conv1d(in_channels=self.d_inner, out_channels=self.d_inner,\n",
        "                                kernel_size=d_conv, groups=self.d_inner, padding=d_conv - 1)\n",
        "        self.x_proj = nn.Linear(self.d_inner, self.dt_rank + d_state * 2, bias=False)\n",
        "        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True)\n",
        "\n",
        "        # Ensure A_log is compatible with varying d_state\n",
        "        A_init = torch.arange(1, d_state + 1, dtype=torch.float32).repeat(self.d_inner, 1)\n",
        "        self.A_log = nn.Parameter(torch.log(A_init))\n",
        "        self.D = nn.Parameter(torch.ones(self.d_inner))\n",
        "        self.out_proj = nn.Linear(self.d_inner, n_embd, bias=False)\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "    def selective_scan(self, u, delta, A, B, C, D):\n",
        "        # A comes in as (d_inner, d_state)\n",
        "        # delta comes in as (batch, seq_len, d_inner)\n",
        "\n",
        "\n",
        "        batch, seq_len, d_inner = u.shape\n",
        "        d_state = A.shape[-1]\n",
        "\n",
        "        #  Explicit reshaping to avoid Einsum ambiguity\n",
        "        # delta: (B, L, D_in) -> (B, L, D_in, 1)\n",
        "        # A: (D_in, D_state) -> (1, 1, D_in, D_state)\n",
        "        # Result: (B, L, D_in, D_state)\n",
        "        deltaA = torch.exp(delta.unsqueeze(-1) * A.view(1, 1, d_inner, d_state))\n",
        "\n",
        "        # B: (B, L, D_state)\n",
        "        # u: (B, L, D_in)\n",
        "        # deltaB_u = delta * B * u\n",
        "        # delta: (B, L, D_in, 1)\n",
        "        # B: (B, L, 1, D_state)\n",
        "        # u: (B, L, D_in, 1)\n",
        "        deltaB_u = delta.unsqueeze(-1) * u.unsqueeze(-1) * B.unsqueeze(2)\n",
        "\n",
        "        x = torch.zeros((batch, d_inner, d_state), device=u.device)\n",
        "        ys = []\n",
        "        for i in range(seq_len):\n",
        "            x = deltaA[:, i] * x + deltaB_u[:, i]\n",
        "            # x: (B, D_in, D_state)\n",
        "            # C: (B, L, D_state) -> C[:, i]: (B, D_state)\n",
        "            # y = x @ C\n",
        "            y = torch.einsum('bdn,bn->bd', x, C[:, i])\n",
        "            ys.append(y)\n",
        "\n",
        "        y = torch.stack(ys, dim=1)\n",
        "        y = y + u * D\n",
        "        return y\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L, E = x.shape\n",
        "        xz = self.in_proj(x)\n",
        "        x_in, z = xz.chunk(2, dim=-1)\n",
        "        x_conv = self.conv1d(x_in.transpose(1, 2))[:, :, :L].transpose(1, 2)\n",
        "        x_conv = self.act(x_conv)\n",
        "        x_dbl = self.x_proj(x_conv)\n",
        "\n",
        "        delta, B_ssm, C_ssm = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=-1)\n",
        "\n",
        "        delta = F.softplus(self.dt_proj(delta))\n",
        "        A = -torch.exp(self.A_log)\n",
        "        y = self.selective_scan(x_conv, delta, A, B_ssm, C_ssm, self.D)\n",
        "        y = y * self.act(z)\n",
        "        return self.out_proj(y)\n",
        "\n",
        "class MambaModel(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embd, block_size, n_layer, d_state=16):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            nn.Sequential(MinimalMambaBlock(n_embd, d_state=d_state), LayerNorm(n_embd, bias=True))\n",
        "            for _ in range(n_layer)\n",
        "        ])\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "        self.block_size = block_size\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        x = self.token_embedding(idx)\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B, T, C = logits.shape\n",
        "            loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
        "        return logits, loss\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "rFNBQKNlCJ3z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. The Training Engine (Strict Data Hygiene)\n",
        "import time\n",
        "import json\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "BLOCK_SIZE = 128\n",
        "BATCH_SIZE = 64  # Shakespeare\n",
        "WORD_BATCH_SIZE = 32 # Wiki/PTB\n",
        "MAX_ITERS = 1000\n",
        "EVAL_INTERVAL = 100\n",
        "LR_CHAR = 1e-3\n",
        "LR_WORD = 5e-4\n",
        "\n",
        "# --- HELPERS ---\n",
        "def get_batch(data_dict, split, block_size, batch_size):\n",
        "    data = data_dict[split]\n",
        "    if len(data) <= block_size:\n",
        "        return data[:-1].unsqueeze(0), data[1:].unsqueeze(0)\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_single_split_loss(model, data_dict, split, block_size, batch_size, eval_iters=20):\n",
        "    # Helper to calculate loss on just ONE split (train OR test)\n",
        "    model.eval()\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "        X, Y = get_batch(data_dict, split, block_size, batch_size)\n",
        "        logits, loss = model(X, Y)\n",
        "        losses[k] = loss.item()\n",
        "    model.train()\n",
        "    return losses.mean().item()\n",
        "\n",
        "def count_flops(model_name, params, steps, batch_size, block_size):\n",
        "    factor = 6\n",
        "    if 'Linear' in model_name or 'MLP' in model_name: factor = 2\n",
        "    if 'Mamba' in model_name: factor = 4\n",
        "    return factor * params * steps * batch_size * block_size\n",
        "\n",
        "def run_experiment(name, model, data_dict, block_size, batch_size, max_iters, lr):\n",
        "    print(f\"--> Training {name}...\")\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    logs = []\n",
        "    n_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "    # --- TRAINING LOOP ---\n",
        "    for iter in range(max_iters):\n",
        "        xb, yb = get_batch(data_dict, 'train', block_size, batch_size)\n",
        "        logits, loss = model(xb, yb)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Log Training Progress\n",
        "        if iter % EVAL_INTERVAL == 0:\n",
        "            train_loss = estimate_single_split_loss(model, data_dict, 'train', block_size, batch_size)\n",
        "            flops = count_flops(name, n_params, iter+1, batch_size, block_size)\n",
        "            logs.append({\n",
        "                'step': iter, 'flops': flops,\n",
        "                'train_loss': train_loss,\n",
        "                'test_loss': np.nan,\n",
        "                'params': n_params\n",
        "            })\n",
        "            print(f\"    Step {iter}: Train Loss {train_loss:.3f}\")\n",
        "\n",
        "    # --- FINAL EVALUATION  ---\n",
        "    print(\"    Training complete. Performing Final Test Set Evaluation...\")\n",
        "    final_test_loss = estimate_single_split_loss(model, data_dict, 'test', block_size, batch_size)\n",
        "    total_flops = count_flops(name, n_params, max_iters, batch_size, block_size)\n",
        "\n",
        "    # Log the final result\n",
        "    logs.append({\n",
        "        'step': max_iters, 'flops': total_flops,\n",
        "        'train_loss': np.nan,\n",
        "        'test_loss': final_test_loss,\n",
        "        'params': n_params\n",
        "    })\n",
        "    print(f\"    >> FINAL RESULTS: Total FLOPs={total_flops:.2e}, Test Loss={final_test_loss:.4f}\")\n",
        "\n",
        "    pd.DataFrame(logs).to_csv(f\"{BASE_DIR}/logs/{name}.csv\", index=False)\n",
        "    torch.save(model.state_dict(), f\"{BASE_DIR}/checkpoints/{name}.pt\")\n",
        "    return final_test_loss\n",
        "\n",
        "# --- 1. HYPERPARAMETER SWEEPS (Tiny Shakespeare) ---\n",
        "print(\"\\n=== DELIVERABLE 1 SWEEPS (Tiny Shakespeare) ===\")\n",
        "sweep_results = {'Linear': [], 'MLP': [], 'SelfAttn': [], 'Transformer': [], 'Mamba': []}\n",
        "\n",
        "# A. Linear\n",
        "for c in [32, 64, 128, 256]:\n",
        "    m = LinearModel(shakespeare['tokenizer'].vocab_size, 128, c).to(device)\n",
        "    loss = run_experiment(f\"Linear_ctx{c}\", m, shakespeare, c, BATCH_SIZE, MAX_ITERS, LR_CHAR)\n",
        "    sweep_results['Linear'].append({'val': c, 'loss': loss})\n",
        "\n",
        "# B. MLP\n",
        "for h in [64, 128, 256, 512]:\n",
        "    m = MLPModel(shakespeare['tokenizer'].vocab_size, 128, BLOCK_SIZE, n_hidden=h).to(device)\n",
        "    loss = run_experiment(f\"MLP_hid{h}\", m, shakespeare, BLOCK_SIZE, BATCH_SIZE, MAX_ITERS, LR_CHAR)\n",
        "    sweep_results['MLP'].append({'val': h, 'loss': loss})\n",
        "\n",
        "# C. Self-Attn\n",
        "for h in [2, 4, 8]:\n",
        "    m = SelfAttentionModel(shakespeare['tokenizer'].vocab_size, 128, BLOCK_SIZE, n_head=h).to(device)\n",
        "    loss = run_experiment(f\"SelfAttn_head{h}\", m, shakespeare, BLOCK_SIZE, BATCH_SIZE, MAX_ITERS, LR_CHAR)\n",
        "    sweep_results['SelfAttn'].append({'val': h, 'loss': loss})\n",
        "\n",
        "# D. Transformer (Layers)\n",
        "for l in [2, 4, 6]:\n",
        "    m = TransformerModel(shakespeare['tokenizer'].vocab_size, 128, BLOCK_SIZE, n_head=4, n_layer=l).to(device)\n",
        "    loss = run_experiment(f\"Transformer_lay{l}\", m, shakespeare, BLOCK_SIZE, BATCH_SIZE, MAX_ITERS, LR_CHAR)\n",
        "    sweep_results['Transformer'].append({'val': l, 'loss': loss})\n",
        "\n",
        "# E. Mamba (State Dim)\n",
        "for s in [8, 16, 32]:\n",
        "    m = MambaModel(shakespeare['tokenizer'].vocab_size, 128, BLOCK_SIZE, n_layer=4, d_state=s).to(device)\n",
        "    loss = run_experiment(f\"Mamba_state{s}\", m, shakespeare, BLOCK_SIZE, BATCH_SIZE, MAX_ITERS, LR_CHAR)\n",
        "    sweep_results['Mamba'].append({'val': s, 'loss': loss})\n",
        "\n",
        "with open(f\"{BASE_DIR}/logs/sweep_summary.json\", 'w') as f: json.dump(sweep_results, f)\n",
        "\n",
        "# --- FIND WINNERS ---\n",
        "best_tf = min(sweep_results['Transformer'], key=lambda x: x['loss'])\n",
        "WINNER_LAYERS = best_tf['val']\n",
        "\n",
        "best_mamba = min(sweep_results['Mamba'], key=lambda x: x['loss'])\n",
        "WINNER_STATE = best_mamba['val']\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\">> TRANSFORMER WINNER: {WINNER_LAYERS} Layers (Loss: {best_tf['loss']:.4f})\")\n",
        "print(f\">> MAMBA WINNER:       State Dim {WINNER_STATE} (Loss: {best_mamba['loss']:.4f})\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "# --- 2. WORD LEVEL BENCHMARKS (Adaptive) ---\n",
        "print(f\"=== DELIVERABLE 2 BENCHMARKS (Adaptive Settings) ===\")\n",
        "datasets = [('WikiText2', wiki), ('PTB', ptb)]\n",
        "\n",
        "for ds_name, ds_data in datasets:\n",
        "    vocab = ds_data['tokenizer'].vocab_size\n",
        "\n",
        "    # 1. Transformer\n",
        "    tf = TransformerModel(vocab, 128, BLOCK_SIZE, n_head=4, n_layer=WINNER_LAYERS).to(device)\n",
        "    run_experiment(f\"{ds_name}_Transformer\", tf, ds_data, BLOCK_SIZE, WORD_BATCH_SIZE, MAX_ITERS, LR_WORD)\n",
        "\n",
        "    # 2. Mamba\n",
        "    ma = MambaModel(vocab, 128, BLOCK_SIZE, n_layer=WINNER_LAYERS, d_state=WINNER_STATE).to(device)\n",
        "    run_experiment(f\"{ds_name}_Mamba\", ma, ds_data, BLOCK_SIZE, WORD_BATCH_SIZE, MAX_ITERS, LR_WORD)\n",
        "\n",
        "print(\"\\nALL TRAINING COMPLETE.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7_k_hqhDZXu",
        "outputId": "d196f948-58f4-4c8e-8436-fa349e79d78d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== DELIVERABLE 1 SWEEPS (Tiny Shakespeare) ===\n",
            "--> Training Linear_ctx32...\n",
            "    Step 0: Train Loss 4.344\n",
            "    Step 100: Train Loss 2.719\n",
            "    Step 200: Train Loss 2.555\n",
            "    Step 300: Train Loss 2.501\n",
            "    Step 400: Train Loss 2.497\n",
            "    Step 500: Train Loss 2.474\n",
            "    Step 600: Train Loss 2.471\n",
            "    Step 700: Train Loss 2.473\n",
            "    Step 800: Train Loss 2.450\n",
            "    Step 900: Train Loss 2.466\n",
            "    Training complete. Performing Final Test Set Evaluation...\n",
            "    >> FINAL RESULTS: Total FLOPs=6.84e+10, Test Loss=2.5150\n",
            "--> Training Linear_ctx64...\n",
            "    Step 0: Train Loss 4.300\n",
            "    Step 100: Train Loss 2.703\n",
            "    Step 200: Train Loss 2.528\n",
            "    Step 300: Train Loss 2.500\n",
            "    Step 400: Train Loss 2.475\n",
            "    Step 500: Train Loss 2.471\n",
            "    Step 600: Train Loss 2.466\n",
            "    Step 700: Train Loss 2.464\n",
            "    Step 800: Train Loss 2.472\n",
            "    Step 900: Train Loss 2.457\n",
            "    Training complete. Performing Final Test Set Evaluation...\n",
            "    >> FINAL RESULTS: Total FLOPs=1.37e+11, Test Loss=2.5149\n",
            "--> Training Linear_ctx128...\n",
            "    Step 0: Train Loss 4.266\n",
            "    Step 100: Train Loss 2.684\n",
            "    Step 200: Train Loss 2.528\n",
            "    Step 300: Train Loss 2.488\n",
            "    Step 400: Train Loss 2.480\n",
            "    Step 500: Train Loss 2.470\n",
            "    Step 600: Train Loss 2.459\n",
            "    Step 700: Train Loss 2.457\n",
            "    Step 800: Train Loss 2.456\n",
            "    Step 900: Train Loss 2.457\n",
            "    Training complete. Performing Final Test Set Evaluation...\n",
            "    >> FINAL RESULTS: Total FLOPs=2.74e+11, Test Loss=2.5040\n",
            "--> Training Linear_ctx256...\n",
            "    Step 0: Train Loss 4.256\n",
            "    Step 100: Train Loss 2.657\n",
            "    Step 200: Train Loss 2.516\n",
            "    Step 300: Train Loss 2.482\n",
            "    Step 400: Train Loss 2.468\n",
            "    Step 500: Train Loss 2.460\n",
            "    Step 600: Train Loss 2.458\n",
            "    Step 700: Train Loss 2.457\n",
            "    Step 800: Train Loss 2.457\n",
            "    Step 900: Train Loss 2.456\n",
            "    Training complete. Performing Final Test Set Evaluation...\n",
            "    >> FINAL RESULTS: Total FLOPs=5.47e+11, Test Loss=2.5059\n",
            "--> Training MLP_hid64...\n",
            "    Step 0: Train Loss 4.179\n",
            "    Step 100: Train Loss 2.576\n",
            "    Step 200: Train Loss 2.499\n",
            "    Step 300: Train Loss 2.479\n",
            "    Step 400: Train Loss 2.474\n",
            "    Step 500: Train Loss 2.470\n",
            "    Step 600: Train Loss 2.464\n",
            "    Step 700: Train Loss 2.462\n",
            "    Step 800: Train Loss 2.464\n",
            "    Step 900: Train Loss 2.459\n",
            "    Training complete. Performing Final Test Set Evaluation...\n",
            "    >> FINAL RESULTS: Total FLOPs=6.13e+11, Test Loss=2.5083\n",
            "--> Training MLP_hid128...\n",
            "    Step 0: Train Loss 4.143\n",
            "    Step 100: Train Loss 2.538\n",
            "    Step 200: Train Loss 2.487\n",
            "    Step 300: Train Loss 2.468\n",
            "    Step 400: Train Loss 2.467\n",
            "    Step 500: Train Loss 2.467\n",
            "    Step 600: Train Loss 2.459\n",
            "    Step 700: Train Loss 2.460\n",
            "    Step 800: Train Loss 2.462\n",
            "    Step 900: Train Loss 2.451\n",
            "    Training complete. Performing Final Test Set Evaluation...\n",
            "    >> FINAL RESULTS: Total FLOPs=1.09e+12, Test Loss=2.5055\n",
            "--> Training MLP_hid256...\n",
            "    Step 0: Train Loss 4.081\n",
            "    Step 100: Train Loss 2.504\n",
            "    Step 200: Train Loss 2.473\n",
            "    Step 300: Train Loss 2.469\n",
            "    Step 400: Train Loss 2.464\n",
            "    Step 500: Train Loss 2.464\n",
            "    Step 600: Train Loss 2.463\n",
            "    Step 700: Train Loss 2.459\n",
            "    Step 800: Train Loss 2.460\n",
            "    Step 900: Train Loss 2.461\n",
            "    Training complete. Performing Final Test Set Evaluation...\n",
            "    >> FINAL RESULTS: Total FLOPs=2.43e+12, Test Loss=2.5086\n",
            "--> Training MLP_hid512...\n",
            "    Step 0: Train Loss 4.017\n",
            "    Step 100: Train Loss 2.486\n",
            "    Step 200: Train Loss 2.470\n",
            "    Step 300: Train Loss 2.461\n",
            "    Step 400: Train Loss 2.463\n",
            "    Step 500: Train Loss 2.469\n",
            "    Step 600: Train Loss 2.467\n",
            "    Step 700: Train Loss 2.454\n",
            "    Step 800: Train Loss 2.465\n",
            "    Step 900: Train Loss 2.457\n",
            "    Training complete. Performing Final Test Set Evaluation...\n",
            "    >> FINAL RESULTS: Total FLOPs=6.74e+12, Test Loss=2.5062\n",
            "--> Training SelfAttn_head2...\n",
            "    Step 0: Train Loss 4.138\n",
            "    Step 100: Train Loss 2.670\n",
            "    Step 200: Train Loss 2.567\n",
            "    Step 300: Train Loss 2.511\n",
            "    Step 400: Train Loss 2.442\n",
            "    Step 500: Train Loss 2.384\n",
            "    Step 600: Train Loss 2.327\n",
            "    Step 700: Train Loss 2.294\n",
            "    Step 800: Train Loss 2.286\n",
            "    Step 900: Train Loss 2.249\n",
            "    Training complete. Performing Final Test Set Evaluation...\n",
            "    >> FINAL RESULTS: Total FLOPs=4.85e+12, Test Loss=2.2965\n",
            "--> Training SelfAttn_head4...\n",
            "    Step 0: Train Loss 4.134\n",
            "    Step 100: Train Loss 2.633\n",
            "    Step 200: Train Loss 2.535\n",
            "    Step 300: Train Loss 2.492\n",
            "    Step 400: Train Loss 2.471\n",
            "    Step 500: Train Loss 2.428\n",
            "    Step 600: Train Loss 2.388\n",
            "    Step 700: Train Loss 2.331\n",
            "    Step 800: Train Loss 2.260\n",
            "    Step 900: Train Loss 2.211\n",
            "    Training complete. Performing Final Test Set Evaluation...\n",
            "    >> FINAL RESULTS: Total FLOPs=4.85e+12, Test Loss=2.2619\n",
            "--> Training SelfAttn_head8...\n",
            "    Step 0: Train Loss 4.117\n",
            "    Step 100: Train Loss 2.626\n",
            "    Step 200: Train Loss 2.530\n",
            "    Step 300: Train Loss 2.481\n",
            "    Step 400: Train Loss 2.450\n",
            "    Step 500: Train Loss 2.418\n",
            "    Step 600: Train Loss 2.388\n",
            "    Step 700: Train Loss 2.347\n",
            "    Step 800: Train Loss 2.299\n",
            "    Step 900: Train Loss 2.255\n",
            "    Training complete. Performing Final Test Set Evaluation...\n",
            "    >> FINAL RESULTS: Total FLOPs=4.85e+12, Test Loss=2.2958\n",
            "--> Training Transformer_lay2...\n",
            "    Step 0: Train Loss 4.023\n",
            "    Step 100: Train Loss 2.505\n",
            "    Step 200: Train Loss 2.384\n",
            "    Step 300: Train Loss 2.283\n",
            "    Step 400: Train Loss 2.163\n",
            "    Step 500: Train Loss 2.060\n",
            "    Step 600: Train Loss 1.959\n",
            "    Step 700: Train Loss 1.879\n",
            "    Step 800: Train Loss 1.819\n",
            "    Step 900: Train Loss 1.766\n",
            "    Training complete. Performing Final Test Set Evaluation...\n",
            "    >> FINAL RESULTS: Total FLOPs=2.11e+13, Test Loss=1.9419\n",
            "--> Training Transformer_lay4...\n",
            "    Step 0: Train Loss 3.834\n",
            "    Step 100: Train Loss 2.481\n",
            "    Step 200: Train Loss 2.334\n",
            "    Step 300: Train Loss 2.189\n",
            "    Step 400: Train Loss 2.050\n",
            "    Step 500: Train Loss 1.928\n",
            "    Step 600: Train Loss 1.838\n",
            "    Step 700: Train Loss 1.773\n",
            "    Step 800: Train Loss 1.705\n",
            "    Step 900: Train Loss 1.658\n",
            "    Training complete. Performing Final Test Set Evaluation...\n",
            "    >> FINAL RESULTS: Total FLOPs=4.05e+13, Test Loss=1.9026\n",
            "--> Training Transformer_lay6...\n",
            "    Step 0: Train Loss 3.716\n",
            "    Step 100: Train Loss 2.456\n",
            "    Step 200: Train Loss 2.317\n",
            "    Step 300: Train Loss 2.129\n",
            "    Step 400: Train Loss 1.997\n",
            "    Step 500: Train Loss 1.874\n",
            "    Step 600: Train Loss 1.768\n",
            "    Step 700: Train Loss 1.692\n",
            "    Step 800: Train Loss 1.635\n",
            "    Step 900: Train Loss 1.592\n",
            "    Training complete. Performing Final Test Set Evaluation...\n",
            "    >> FINAL RESULTS: Total FLOPs=6.00e+13, Test Loss=1.8640\n",
            "--> Training Mamba_state8...\n",
            "    Step 0: Train Loss 3.984\n",
            "    Step 100: Train Loss 1.654\n",
            "    Step 200: Train Loss 1.496\n",
            "    Step 300: Train Loss 1.432\n",
            "    Step 400: Train Loss 1.387\n",
            "    Step 500: Train Loss 1.348\n",
            "    Step 600: Train Loss 1.327\n",
            "    Step 700: Train Loss 1.318\n",
            "    Step 800: Train Loss 1.296\n",
            "    Step 900: Train Loss 1.280\n",
            "    Training complete. Performing Final Test Set Evaluation...\n",
            "    >> FINAL RESULTS: Total FLOPs=1.50e+13, Test Loss=1.8126\n",
            "--> Training Mamba_state16...\n",
            "    Step 0: Train Loss 3.964\n",
            "    Step 100: Train Loss 1.656\n",
            "    Step 200: Train Loss 1.507\n",
            "    Step 300: Train Loss 1.434\n",
            "    Step 400: Train Loss 1.386\n",
            "    Step 500: Train Loss 1.362\n",
            "    Step 600: Train Loss 1.334\n",
            "    Step 700: Train Loss 1.319\n",
            "    Step 800: Train Loss 1.299\n",
            "    Step 900: Train Loss 1.295\n",
            "    Training complete. Performing Final Test Set Evaluation...\n",
            "    >> FINAL RESULTS: Total FLOPs=1.58e+13, Test Loss=1.8006\n",
            "--> Training Mamba_state32...\n",
            "    Step 0: Train Loss 3.990\n",
            "    Step 100: Train Loss 1.641\n",
            "    Step 200: Train Loss 1.489\n",
            "    Step 300: Train Loss 1.422\n",
            "    Step 400: Train Loss 1.366\n",
            "    Step 500: Train Loss 1.352\n",
            "    Step 600: Train Loss 1.327\n",
            "    Step 700: Train Loss 1.313\n",
            "    Step 800: Train Loss 1.296\n",
            "    Step 900: Train Loss 1.290\n",
            "    Training complete. Performing Final Test Set Evaluation...\n",
            "    >> FINAL RESULTS: Total FLOPs=1.75e+13, Test Loss=1.8213\n",
            "\n",
            "==================================================\n",
            ">> TRANSFORMER WINNER: 6 Layers (Loss: 1.8640)\n",
            ">> MAMBA WINNER:       State Dim 16 (Loss: 1.8006)\n",
            "==================================================\n",
            "\n",
            "=== DELIVERABLE 2 BENCHMARKS (Adaptive Settings) ===\n",
            "--> Training WikiText2_Transformer...\n",
            "    Step 0: Train Loss 10.382\n",
            "    Step 100: Train Loss 6.881\n",
            "    Step 200: Train Loss 6.608\n",
            "    Step 300: Train Loss 6.451\n",
            "    Step 400: Train Loss 6.311\n",
            "    Step 500: Train Loss 6.228\n",
            "    Step 600: Train Loss 6.117\n",
            "    Step 700: Train Loss 6.043\n",
            "    Step 800: Train Loss 5.948\n",
            "    Step 900: Train Loss 5.863\n",
            "    Training complete. Performing Final Test Set Evaluation...\n",
            "    >> FINAL RESULTS: Total FLOPs=2.40e+14, Test Loss=5.6174\n",
            "--> Training WikiText2_Mamba...\n",
            "    Step 0: Train Loss 10.518\n",
            "    Step 100: Train Loss 6.877\n",
            "    Step 200: Train Loss 6.612\n",
            "    Step 300: Train Loss 6.399\n",
            "    Step 400: Train Loss 6.220\n",
            "    Step 500: Train Loss 6.096\n",
            "    Step 600: Train Loss 5.954\n",
            "    Step 700: Train Loss 5.870\n",
            "    Step 800: Train Loss 5.749\n",
            "    Step 900: Train Loss 5.727\n",
            "    Training complete. Performing Final Test Set Evaluation...\n",
            "    >> FINAL RESULTS: Total FLOPs=1.52e+14, Test Loss=5.5213\n",
            "--> Training PTB_Transformer...\n",
            "    Step 0: Train Loss 9.169\n",
            "    Step 100: Train Loss 6.425\n",
            "    Step 200: Train Loss 6.172\n",
            "    Step 300: Train Loss 5.940\n",
            "    Step 400: Train Loss 5.761\n",
            "    Step 500: Train Loss 5.669\n",
            "    Step 600: Train Loss 5.531\n",
            "    Step 700: Train Loss 5.485\n",
            "    Step 800: Train Loss 5.394\n",
            "    Step 900: Train Loss 5.299\n",
            "    Training complete. Performing Final Test Set Evaluation...\n",
            "    >> FINAL RESULTS: Total FLOPs=9.27e+13, Test Loss=5.3223\n",
            "--> Training PTB_Mamba...\n",
            "    Step 0: Train Loss 9.362\n",
            "    Step 100: Train Loss 6.501\n",
            "    Step 200: Train Loss 6.198\n",
            "    Step 300: Train Loss 5.948\n",
            "    Step 400: Train Loss 5.771\n",
            "    Step 500: Train Loss 5.579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# --- SETUP ---\n",
        "BASE_DIR = '/content/drive/My Drive/HW6_Project'\n",
        "IMG_DIR = f\"{BASE_DIR}/images\"\n",
        "LOG_DIR = f\"{BASE_DIR}/logs\"\n",
        "os.makedirs(IMG_DIR, exist_ok=True)\n",
        "\n",
        "# load sweep data\n",
        "with open(f\"{LOG_DIR}/sweep_summary.json\", 'r') as f:\n",
        "    sweeps = json.load(f)\n",
        "\n",
        "colors = {'Linear': 'tab:blue', 'MLP': 'tab:orange', 'SelfAttn': 'tab:green',\n",
        "          'Transformer': 'tab:red', 'Mamba': 'tab:purple'}\n",
        "markers = {'Linear': 'o', 'MLP': 's', 'SelfAttn': '^', 'Transformer': 'D', 'Mamba': 'v'}\n",
        "\n",
        "# identify best configs for the combined plots\n",
        "best_files = {}\n",
        "for model_name, results in sweeps.items():\n",
        "    best_run = min(results, key=lambda x: x['loss'])\n",
        "    val = best_run['val']\n",
        "\n",
        "    if model_name == 'Linear': fname = f\"Linear_ctx{val}\"\n",
        "    elif model_name == 'MLP': fname = f\"MLP_hid{val}\"\n",
        "    elif model_name == 'SelfAttn': fname = f\"SelfAttn_head{val}\"\n",
        "    elif model_name == 'Transformer': fname = f\"Transformer_lay{val}\"\n",
        "    elif model_name == 'Mamba': fname = f\"Mamba_state{val}\"\n",
        "    best_files[model_name] = fname\n",
        "\n",
        "print(\"Best Configurations Found:\")\n",
        "for k, v in best_files.items(): print(f\"  {k}: {v}\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. DELIVERABLE: plot_1_shakespeare_train.png (Combined Training Convergence)\n",
        "# ==============================================================================\n",
        "print(\"\\nGenerating Plot 1: Shakespeare Training Convergence...\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for model_label, filename in best_files.items():\n",
        "    try:\n",
        "        df = pd.read_csv(f\"{LOG_DIR}/{filename}.csv\")\n",
        "        train_df = df.dropna(subset=['train_loss'])\n",
        "\n",
        "        plt.plot(train_df['epoch'], train_df['train_loss'],\n",
        "                 label=model_label, color=colors[model_label], linewidth=2, alpha=0.8)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"  Warning: {filename} not found.\")\n",
        "\n",
        "plt.xlabel('Effective Epochs', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Training Loss', fontsize=12, fontweight='bold')\n",
        "plt.title('Training Convergence on Tiny Shakespeare', fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{IMG_DIR}/plot_1_shakespeare_train.png\", dpi=300)\n",
        "plt.close()\n",
        "print(f\"  Saved: {IMG_DIR}/plot_1_shakespeare_train.png\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. DELIVERABLE: plot_2_hyperparam_sensitivity.png (5-Panel Figure)\n",
        "# ==============================================================================\n",
        "print(\"\\nGenerating Plot 2: Hyperparameter Sensitivity...\")\n",
        "fig, axes = plt.subplots(1, 5, figsize=(20, 4), sharey=True)\n",
        "\n",
        "# 5 subplots\n",
        "configs = [\n",
        "    ('Linear', 'Context Size', 'Linear'),\n",
        "    ('MLP', 'Hidden Dim', 'MLP'),\n",
        "    ('SelfAttn', 'Heads', 'SelfAttn'),\n",
        "    ('Transformer', 'Layers', 'Transformer'),\n",
        "    ('Mamba', 'State Dim', 'Mamba')\n",
        "]\n",
        "\n",
        "for i, (model_key, x_label, title) in enumerate(configs):\n",
        "    ax = axes[i]\n",
        "    data = sweeps[model_key] # list of dicts: {'val': 32, 'loss': 2.5}\n",
        "\n",
        "    # sort by hyperparam value for clean plotting\n",
        "    data.sort(key=lambda x: x['val'])\n",
        "\n",
        "    x_vals = [d['val'] for d in data]\n",
        "    y_vals = [d['loss'] for d in data]\n",
        "\n",
        "    ax.plot(x_vals, y_vals, marker='o', linestyle='-', color=colors[model_key], linewidth=2)\n",
        "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
        "    ax.set_xlabel(x_label, fontsize=10)\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # force integer ticks for discrete params (Layers, Heads)\n",
        "    if title in ['Transformer', 'SelfAttn']:\n",
        "        ax.set_xticks(x_vals)\n",
        "\n",
        "axes[0].set_ylabel('Final Test Set Loss', fontsize=12, fontweight='bold')\n",
        "plt.suptitle('Hyperparameter Sensitivity Analysis', fontsize=16, y=1.05)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{IMG_DIR}/plot_2_hyperparam_sensitivity.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"  Saved: {IMG_DIR}/plot_2_hyperparam_sensitivity.png\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. DELIVERABLE: plot_3_compute_efficiency.png (Efficiency Scatter)\n",
        "# ==============================================================================\n",
        "print(\"\\nGenerating Plot 3: Compute Efficiency...\")\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "for model_label, filename in best_files.items():\n",
        "    try:\n",
        "        df = pd.read_csv(f\"{LOG_DIR}/{filename}.csv\")\n",
        "        final_row = df.iloc[-1]\n",
        "\n",
        "        # plot scatter point\n",
        "        plt.scatter(final_row['flops'], final_row['test_loss'],\n",
        "                    color=colors[model_label], marker=markers[model_label], s=250,\n",
        "                    label=model_label, edgecolors='black', zorder=5)\n",
        "\n",
        "        # label text near the dot\n",
        "        plt.text(final_row['flops'], final_row['test_loss'] - 0.05,\n",
        "                 f\"{final_row['test_loss']:.2f}\", ha='center', fontsize=9)\n",
        "\n",
        "    except FileNotFoundError: continue\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Training FLOPs (Log Scale)', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Final Test Set Loss', fontsize=12, fontweight='bold')\n",
        "plt.title('Compute Efficiency Frontier', fontsize=14)\n",
        "plt.legend(title=\"Best Configurations\")\n",
        "plt.grid(True, which=\"both\", linestyle='--', alpha=0.4)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{IMG_DIR}/plot_3_compute_efficiency.png\", dpi=300)\n",
        "plt.close()\n",
        "print(f\"  Saved: {IMG_DIR}/plot_3_compute_efficiency.png\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. INDIVIDUAL PLOTS\n",
        "# ==============================================================================\n",
        "print(\"\\nGenerating Individual Backup Plots...\")\n",
        "for model_label, filename in best_files.items():\n",
        "    try:\n",
        "        df = pd.read_csv(f\"{LOG_DIR}/{filename}.csv\")\n",
        "        train_df = df.dropna(subset=['train_loss'])\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(train_df['epoch'], train_df['train_loss'], color=colors[model_label])\n",
        "        plt.title(f\"{model_label} Training Curve\")\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"Train Loss\")\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.savefig(f\"{IMG_DIR}/individual_train_{model_label}.png\")\n",
        "        plt.close()\n",
        "    except: pass\n",
        "\n",
        "print(\"\\nALL PLOTS GENERATED.\")"
      ],
      "metadata": {
        "id": "HLvcj7vys1q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5. Emergency Resume (Run ONLY if Runtime died during Benchmarks)\n",
        "# Run Block 1 & 2 first!\n",
        "import json\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Load the winner configs from the saved JSON\n",
        "with open(f\"{BASE_DIR}/logs/sweep_summary.json\", 'r') as f:\n",
        "    sweep_results = json.load(f)\n",
        "\n",
        "best_tf = min(sweep_results['Transformer'], key=lambda x: x['loss'])\n",
        "WINNER_LAYERS = best_tf['val']\n",
        "\n",
        "best_mamba = min(sweep_results['Mamba'], key=lambda x: x['loss'])\n",
        "WINNER_STATE = best_mamba['val']\n",
        "\n",
        "print(f\"Resuming with Winners -> Transformer Layers: {WINNER_LAYERS}, Mamba State: {WINNER_STATE}\")\n",
        "\n",
        "# Benchmarks\n",
        "datasets = [('WikiText2', wiki), ('PTB', ptb)]\n",
        "WORD_BATCH_SIZE = 32\n",
        "MAX_ITERS = 1000\n",
        "LR_WORD = 5e-4\n",
        "\n",
        "for ds_name, ds_data in datasets:\n",
        "    vocab = ds_data['tokenizer'].vocab_size\n",
        "\n",
        "    # Check if Transformer is already done\n",
        "    if os.path.exists(f\"{BASE_DIR}/logs/{ds_name}_Transformer.csv\"):\n",
        "        print(f\"Skipping {ds_name}_Transformer (Already Done)\")\n",
        "    else:\n",
        "        tf = TransformerModel(vocab, 128, BLOCK_SIZE, n_head=4, n_layer=WINNER_LAYERS).to(device)\n",
        "        run_experiment(f\"{ds_name}_Transformer\", tf, ds_data, BLOCK_SIZE, WORD_BATCH_SIZE, MAX_ITERS, LR_WORD)\n",
        "\n",
        "    # Check if Mamba is already done\n",
        "    if os.path.exists(f\"{BASE_DIR}/logs/{ds_name}_Mamba.csv\"):\n",
        "         print(f\"Skipping {ds_name}_Mamba (Already Done)\")\n",
        "    else:\n",
        "        ma = MambaModel(vocab, 128, BLOCK_SIZE, n_layer=WINNER_LAYERS, d_state=WINNER_STATE).to(device)\n",
        "        run_experiment(f\"{ds_name}_Mamba\", ma, ds_data, BLOCK_SIZE, WORD_BATCH_SIZE, MAX_ITERS, LR_WORD)\n",
        "\n",
        "print(\"All Benchmarks Complete.\")"
      ],
      "metadata": {
        "id": "8b5XIbhxQ94V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___"
      ],
      "metadata": {
        "id": "cgkoaSwUONtT"
      }
    }
  ]
}